from selenium.common import NoSuchElementException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from dotenv import load_dotenv
import logging
import os
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from urllib.parse import urljoin

logger = logging.getLogger(__name__)


class GanaswimCrawler:
    """ê°€ë‚˜ìŠ¤ìœ” ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""

    def __init__(self, headless=True):
        """
        Args:
            headless: Trueë©´ ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
        """
        self.driver = None
        self.headless = headless
        self.product_list = []
        load_dotenv()

    def setup_driver(self):
        """í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì • ë° ì‹¤í–‰"""
        options = webdriver.ChromeOptions()

        # 1. ë¸Œë¼ìš°ì €(Chrome) ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        # os.getenv('CHROME_PATH')ê°€ /usr/bin/google-chrome ë¼ë©´ ì—¬ê¸°ì— í• ë‹¹í•©ë‹ˆë‹¤.
        chrome_bin = os.getenv('CHROME_PATH')
        options.binary_location = chrome_bin

        options.add_argument('--headless=new')  # ìµœì‹  í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ (ë” ë¹ ë¦„)
        options.add_argument('--window-size=1920,1080')  # ë„ì»¤ í™˜ê²½ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')

        # [í•µì‹¬] ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨: ì´ë¯¸ì§€, CSS, í°íŠ¸ ë¡œë”© ë°©ì§€ (CPU ë‚­ë¹„ ë°©ì§€)
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.managed_default_content_settings.stylesheets": 2,
            "profile.managed_default_content_settings.fonts": 2,
        }
        options.add_experimental_option("prefs", prefs)

        # ìë™í™” íƒì§€ ë°©ì§€ (UC ëŒ€ì‹  ê°€ë²¼ìš´ ì˜µì…˜)
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)

        # Serviceì—ëŠ” 'ë“œë¼ì´ë²„' ê²½ë¡œë¥¼ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤.
        service = Service()
        self.driver = webdriver.Chrome(service=service, options=options)

        # ì‹¤í–‰ ì†ë„ í–¥ìƒì„ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (Webdriver ì†ì„± ì œê±°)
        self.driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": """
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    })
                """
        })

    def quit_driver(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            logger.info("âœ“ ë“œë¼ì´ë²„ ì¢…ë£Œë¨")

    def get_end_page(self, pageDiv):
        """ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            buttons = pageDiv.find_all('button')
            pageLastButton = buttons[-1]
            endPage = int(pageLastButton.find('span').get_text(strip=True))
            return endPage
        except Exception as e:
            logger.exception("í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: %s", e)
            return 1

    def wait_for_load(self):
        """í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°"""
        try:
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.CLASS_NAME, 'sc-2667f19f-45'))
            )
            return True
        except Exception as e:
            logger.exception("í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: %s", e)
            return False

    def extract_product_info_bs4(self, element):
        """
        BeautifulSoup ê°ì²´(element)ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì´ˆê³ ì† ë¡œì§
        """
        try:
            base_url = "https://swim.co.kr"
            # 1. ë§í¬ ì¶”ì¶œ (find_element ëŒ€ì‹  .findë‚˜ .select ì‚¬ìš©)
            link_tag = element.find('a')
            product_url = urljoin(base_url, link_tag['href']) if link_tag else ""

            # 2. ë¸Œëœë“œ, ìƒí’ˆëª… ì¶”ì¶œ (innerText ëŒ€ì‹  .get_text())
            brand_tag = element.select_one('.dVHoSm')
            brand = brand_tag.get_text(strip=True) if brand_tag else "ì•Œ ìˆ˜ ì—†ìŒ"

            name_tag = element.select_one('.cjytLO')
            name = name_tag.get_text(strip=True) if name_tag else "ìƒí’ˆëª… ì—†ìŒ"

            # 3. ê°€ê²© ì¶”ì¶œ (ë³µì¡í•œ span êµ¬ì¡°ë„ í…ìŠ¤íŠ¸ë¡œ í•œ ë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥)
            price_span = element.select('.dVHoSm')
            if price_span[-1]:
                # í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ìˆ«ìë§Œ ê³¨ë¼ë‚´ê¸° (ì›, , ì œê±°)
                raw_price = price_span[-1].get_text().strip()
                # ê°€ì¥ ë’¤ì— ìˆëŠ” ìˆ«ìê°€ ì‹¤ì œ ê°€ê²©ì¸ ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ ì²˜ë¦¬
                price = "".join(filter(str.isdigit, raw_price))
            else:
                price = "0"

            # 4. ì´ë¯¸ì§€ URL ì¶”ì¶œ
            img_tag = element.find('img')
            img_url = urljoin(base_url, img_tag['src']) if img_tag else ""

            # 5. í’ˆì ˆ ì—¬ë¶€ í™•ì¸ (í´ë˜ìŠ¤ ì¡´ì¬ ì—¬ë¶€ë§Œ ì²´í¬)
            is_sold_out = True if element.select_one('.sc-eef3f2e7-3') else False

            return {
                "brand": brand,
                "name": name,
                "price": price,
                "product_url": product_url,
                "img_url": img_url,
                "is_sold_out": is_sold_out
            }

        except Exception as e:
            # ì—ëŸ¬ ë¡œê·¸ëŠ” ë‚¨ê¸°ë˜ ì „ì²´ ë£¨í”„ê°€ ê¹¨ì§€ì§€ ì•Šê²Œ ì²˜ë¦¬
            logger.debug(f"ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì¤‘ ê±´ë„ˆëœ€: {e}")
            return None

    def crawl_page(self, elements):
        """í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            logger.info(f"ğŸ“¦ ë°œê²¬ëœ ìƒí’ˆ ìˆ˜: {len(elements)}")

            for element in elements:
                product_info = self.extract_product_info_bs4(element)
                self.product_list.append(product_info)

            return True

        except Exception as e:
            logger.exception(f"BS4 íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

    def crawl(self, url):
        """
        í¬ë¡¤ë§ ì‹¤í–‰ (ë©”ì¸ ë©”ì„œë“œ)

        Args:
            url: í¬ë¡¤ë§í•  ê¸°ë³¸ URL (pageNumber íŒŒë¼ë¯¸í„° ì œì™¸)

        Returns:
            list: ì¶”ì¶œëœ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("ğŸš€ í¬ë¡¤ë§ ì‹œì‘...")

        # ë“œë¼ì´ë²„ ì„¤ì •
        self.setup_driver()
        self.product_list = []

        clean_url = url.split("&pageNumber=1")[0] if "&pageNumber=1" in url else url

        try:
            current_page = 1

            while True:
                logger.debug(f"\nğŸ“„ í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì¤‘...")

                # URL ì ‘ì†
                full_url = f"{clean_url}&pageNumber={current_page}"
                logger.info(f"##### í˜„ì¬ url {full_url}")

                self.driver.get(full_url)

                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                # --- ì¬ì‹œë„ ì—†ì´ ë°”ë¡œ ì²´í¬ ---
                if not self.wait_for_load():
                    # ì—¬ê¸°ì„œ ë°”ë¡œ ì—ëŸ¬ë¥¼ ë˜ì§€ë©´ finallyë¡œ ê°€ì„œ ë“œë¼ì´ë²„ ë„ê³  ëë‚¨!
                    raise Exception(f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ (URL: {full_url})")

                # 1. ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸° (Selenium í†µì‹  1íšŒ)
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')

                # 2. ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
                elements = soup.select('.cGXxzj')  # ë§ˆì¹¨í‘œ(.) í•„ìˆ˜!

                # 3. ë°ì´í„° íŒŒì‹± ì‹¤í–‰ (ë©”ëª¨ë¦¬ ì—°ì‚°ì´ë¼ ê´‘ì†!)
                if not self.crawl_page(elements):
                    break

                # 4. ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸ ë¡œì§ (í´ë˜ìŠ¤ëª… ì„ íƒ ì£¼ì˜)
                pageDiv = soup.find(class_='sc-b97ceab4-2')
                if pageDiv:
                    end_page = self.get_end_page(pageDiv)
                else:
                    end_page = current_page  # ëª» ì°¾ìœ¼ë©´ í˜„ì¬ í˜ì´ì§€ë¥¼ ë§ˆì§€ë§‰ìœ¼ë¡œ ê°„ì£¼

                if current_page >= end_page:
                    logger.info(f"âœ“ ë§ˆì§€ë§‰ í˜ì´ì§€({end_page}) ë„ë‹¬")
                    break

                current_page += 1

        except Exception as e:
            logger.exception(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:  {e}")

            # â­ í•µì‹¬: ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë˜ì ¸ì•¼ ì•„ë˜ return ë¬¸ìœ¼ë¡œ ì•ˆ ë‚´ë ¤ê°€!
            raise e

        finally:
            self.quit_driver()

        logger.info(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ {len(self.product_list)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")

        return self.product_list


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê¸°ë³¸ ì‚¬ìš© (ë¸Œë¼ìš°ì € ì•ˆë³´ì„)
    crawler = GanaswimCrawler(headless=True)

    url = "https://swim.co.kr/categories/918606/products?childCategoryNo=919019&brands=%255B43160576%255D&pageNumber=1"
    product_list = crawler.crawl(url)

    # ê²°ê³¼ ì¶œë ¥
    for i, product in enumerate(product_list, 1):
        print(f"{i}. {product}")

# ë°°ëŸ´
# https://swim.co.kr/categories/918606/products?childCategoryNo=919019&brands=%255B43160576%255D&pageNumber=1

# í”¼ë‹‰ìŠ¤
# https://swim.co.kr/categories/918606/products?childCategoryNo=919019&brands=%255B43160578%255D
