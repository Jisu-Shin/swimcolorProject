from selenium. common import NoSuchElementException
from selenium. webdriver.common.by import By
from selenium. webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import undetected_chromedriver as uc
import os
from dotenv import load_dotenv
import logging

logger = logging.getLogger(__name__)

class SwimcapCrawler:
    """ìˆ˜ëª¨ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""

    def __init__(self, headless=True):
        """
        Args:
            headless: Trueë©´ ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
        """
        self.driver = None
        self.headless = headless
        self.product_list = []
        load_dotenv()

    def setup_driver(self):
        """í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì • ë° ì‹¤í–‰"""
        options = uc.ChromeOptions()

        # ë¦¬ëˆ…ìŠ¤ ì„œë²„ í™˜ê²½ í•„ìˆ˜ ì˜µì…˜
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')  # ì¤‘ìš”!
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')

        chrome_path = os.getenv('CHROME_PATH')  # ê¸°ë³¸ê°’ ì„¤ì •

        print(f"--- ë“œë¼ì´ë²„ ì‹¤í–‰ ì‹œë„ (Path: {chrome_path}) ---")

        try:
            self.driver = uc.Chrome(
                options=options,
                browser_executable_path=chrome_path,
                headless=self.headless,  # optionsì— ë„£ì§€ ë§ê³  ì—¬ê¸°ì— ì§ì ‘!
                use_subprocess=True  # ë¦¬ëˆ…ìŠ¤ í™˜ê²½ì—ì„œ ì¶©ëŒ ë°©ì§€ í•µì‹¬
            )
            print("--- ë“œë¼ì´ë²„ ì‹¤í–‰ ì„±ê³µ! ---")
        except Exception as e:
            print(f"--- ë“œë¼ì´ë²„ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)} ---")
            raise e

    def quit_driver(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            logger.info("âœ“ ë“œë¼ì´ë²„ ì¢…ë£Œë¨")

    def get_end_page(self):
        """ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            pageDiv = self.driver.find_element(By.CLASS_NAME, 'sc-b97ceab4-2')
            pageLastButton = pageDiv.find_elements(By.TAG_NAME, 'button')[-1]
            endPage = int(pageLastButton.find_element(By.TAG_NAME, 'span').text)
            return endPage
        except Exception as e:
            logger.exception("í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: %s", e)
            return 1

    def wait_for_load(self):
        """í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°"""
        try:
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.CLASS_NAME, 'sc-2667f19f-45'))
            )
            return True
        except Exception as e:
            logger.exception("í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: %s", e)
            return False

    def extract_product_info(self, element):
        """
        ê°œë³„ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ

        Returns:
            dict: ìƒí’ˆ ì •ë³´ ë˜ëŠ” None (ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ)
        """
        try:
            # ë§í¬ ì¶”ì¶œ
            product_url = element.find_element(By.TAG_NAME, 'a').get_attribute('href')

            # ìƒí’ˆëª…, ë¸Œëœë“œ, ê°€ê²© ì¶”ì¶œ
            desc = element.find_element(By.CLASS_NAME, 'kIsRDZ').text

            desc_split = desc.split('\n')
            brand = desc_split[0]
            name = desc_split[1]
            price = desc_split[2] if len(desc_split) == 3 else desc_split[3]

            price = price.replace(",", "").replace("ì›", "")

            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            img_url = element.find_element(By.TAG_NAME, 'img').get_attribute('src')

            # í’ˆì ˆ ì—¬ë¶€ í™•ì¸
            try:
                element.find_element(By.CLASS_NAME, 'sc-eef3f2e7-3')
                is_sold_out = True
            except NoSuchElementException:
                is_sold_out = False

            return {
                "brand": brand,
                "name": name,
                "price": price,
                "product_url": product_url,
                "img_url": img_url,
                "is_sold_out": is_sold_out
            }

        except Exception as e:
            logger.exception("ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: %s", e)
            return None

    def crawl_page(self):
        """í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            elements = self.driver.find_elements(By.CLASS_NAME, 'cGXxzj')
            logger.info(f"ğŸ“¦ ë°œê²¬ëœ ìƒí’ˆ ìˆ˜:  {len(elements)}")

            for element in elements:
                product_info = self.extract_product_info(element)

                if product_info:
                    if product_info['is_sold_out']:
                        logger.debug(f"  âœ— [í’ˆì ˆ] {product_info['brand']} - {product_info['name']}")
                    else:
                        # print(f"  âœ“ {product_info['brand']} - {product_info['name']}")
                        self.product_list.append(product_info)

            return True

        except Exception as e:
            logger.exception(f"í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return False

    def crawl(self, url):
        """
        í¬ë¡¤ë§ ì‹¤í–‰ (ë©”ì¸ ë©”ì„œë“œ)

        Args:
            url: í¬ë¡¤ë§í•  ê¸°ë³¸ URL (pageNumber íŒŒë¼ë¯¸í„° ì œì™¸)

        Returns:
            list: ì¶”ì¶œëœ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("ğŸš€ í¬ë¡¤ë§ ì‹œì‘...")

        # ë“œë¼ì´ë²„ ì„¤ì •
        self.setup_driver()
        self.product_list = []

        try:
            current_page = 1

            while True:
                logger.debug(f"\nğŸ“„ í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì¤‘...")

                # URL ì ‘ì†
                full_url = f"{url}&pageNumber={current_page}"
                self.driver.get(full_url)

                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                # --- ì¬ì‹œë„ ì—†ì´ ë°”ë¡œ ì²´í¬ ---
                if not self.wait_for_load():
                    # ì—¬ê¸°ì„œ ë°”ë¡œ ì—ëŸ¬ë¥¼ ë˜ì§€ë©´ finallyë¡œ ê°€ì„œ ë“œë¼ì´ë²„ ë„ê³  ëë‚¨!
                    raise Exception(f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ (URL: {full_url})")

                # í˜„ì¬ í˜ì´ì§€ì˜ ìƒí’ˆ í¬ë¡¤ë§
                if not self.crawl_page():
                    break

                # ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸
                end_page = self.get_end_page()

                if current_page >= end_page:
                    logger.debug(f"âœ“ ë§ˆì§€ë§‰ í˜ì´ì§€({end_page})ì— ë„ë‹¬")
                    break

                current_page += 1
                time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

        except Exception as e:
            logger.exception(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:  {e}")

            # â­ í•µì‹¬: ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë˜ì ¸ì•¼ ì•„ë˜ return ë¬¸ìœ¼ë¡œ ì•ˆ ë‚´ë ¤ê°€!
            raise e

        finally:
            self.quit_driver()

        logger.info(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ {len(self.product_list)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")

        return self.product_list

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê¸°ë³¸ ì‚¬ìš© (ë¸Œë¼ìš°ì € ì•ˆë³´ì„)
    crawler = SwimcapCrawler(headless=True)

    url = "https://swim.co.kr/categories/918606/products?childCategoryNo=919019&categoryNos=%255B%2522923110%2522%255D&pageNumber=1"

    product_list = crawler.crawl(url)

    # ê²°ê³¼ ì¶œë ¥
    for i, product in enumerate(product_list, 1):
        print(f"{i}. {product}")

# ë°°ëŸ´
# https://swim.co.kr/categories/918606/products?childCategoryNo=919019&brands=%255B43160576%255D&pageNumber=1

# í”¼ë‹‰ìŠ¤
# https://swim.co.kr/categories/918606/products?childCategoryNo=919019&brands=%255B43160578%255D