from selenium. common import NoSuchElementException
from selenium. webdriver.common.by import By
from selenium. webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import undetected_chromedriver as uc

class SwimcapCrawler:
    """ìˆ˜ëª¨ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""

    def __init__(self, headless=True):
        """
        Args:
            headless: Trueë©´ ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
        """
        self.driver = None
        self.headless = headless
        self.product_list = []

    def setup_driver(self):
        """í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì • ë° ì‹¤í–‰"""
        options = uc.ChromeOptions()

        if self.headless:
            options.add_argument('--headless=new')  # ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ

        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--window-size=1920,1080')

        self.driver = uc.Chrome(
            options=options
            , browser_executable_path="/usr/bin/google-chrome"
        )

    def quit_driver(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("âœ“ ë“œë¼ì´ë²„ ì¢…ë£Œë¨")

    def get_end_page(self):
        """ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            pageDiv = self.driver.find_element(By.CLASS_NAME, 'sc-b97ceab4-2')
            pageLastButton = pageDiv.find_elements(By.TAG_NAME, 'button')[-1]
            endPage = int(pageLastButton.find_element(By.TAG_NAME, 'span').text)
            return endPage
        except Exception as e:
            print(f"í˜ì´ì§€ ë²ˆí˜¸ ì¡°íšŒ ì‹¤íŒ¨:  {e}")
            return 1

    def wait_for_load(self):
        """í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°"""
        try:
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.CLASS_NAME, 'sc-2667f19f-45'))
            )
            return True
        except Exception as e:
            print(f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    def extract_product_info(self, element):
        """
        ê°œë³„ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ

        Returns:
            dict: ìƒí’ˆ ì •ë³´ ë˜ëŠ” None (ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ)
        """
        try:
            # ë§í¬ ì¶”ì¶œ
            product_url = element.find_element(By.TAG_NAME, 'a').get_attribute('href')

            # ìƒí’ˆëª…, ë¸Œëœë“œ, ê°€ê²© ì¶”ì¶œ
            desc = element.find_element(By.CLASS_NAME, 'kIsRDZ').text

            desc_split = desc.split('\n')
            brand = desc_split[0]
            name = desc_split[1]
            price = desc_split[2] if len(desc_split) == 3 else desc_split[3]

            price = price.replace(",", "").replace("ì›", "")

            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            img_url = element.find_element(By.TAG_NAME, 'img').get_attribute('src')

            # í’ˆì ˆ ì—¬ë¶€ í™•ì¸
            try:
                element.find_element(By.CLASS_NAME, 'sc-eef3f2e7-3')
                is_sold_out = True
            except NoSuchElementException:
                is_sold_out = False

            return {
                "brand": brand,
                "name": name,
                "price": price,
                "product_url": product_url,
                "img_url": img_url,
                "is_sold_out": is_sold_out
            }

        except Exception as e:
            print(f"ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨:  {e}")
            return None

    def crawl_page(self):
        """í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            elements = self.driver.find_elements(By.CLASS_NAME, 'cGXxzj')
            print(f"ğŸ“¦ ë°œê²¬ëœ ìƒí’ˆ ìˆ˜:  {len(elements)}")

            for element in elements:
                product_info = self.extract_product_info(element)

                if product_info:
                    if product_info['is_sold_out']:
                        print(f"  âœ— [í’ˆì ˆ] {product_info['brand']} - {product_info['name']}")
                    else:
                        # print(f"  âœ“ {product_info['brand']} - {product_info['name']}")
                        self.product_list.append(product_info)

            return True

        except Exception as e:
            print(f"í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return False

    def crawl(self, url):
        # print("\n\ncrawlì—ì„œ í™•ì¸", url);
        """
        í¬ë¡¤ë§ ì‹¤í–‰ (ë©”ì¸ ë©”ì„œë“œ)

        Args:
            url: í¬ë¡¤ë§í•  ê¸°ë³¸ URL (pageNumber íŒŒë¼ë¯¸í„° ì œì™¸)

        Returns:
            list: ì¶”ì¶œëœ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸
        """
        print("ğŸš€ í¬ë¡¤ë§ ì‹œì‘...")

        # ë“œë¼ì´ë²„ ì„¤ì •
        self.setup_driver()
        self.product_list = []

        try:
            current_page = 1

            while True:
                print(f"\nğŸ“„ í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì¤‘...")

                # URL ì ‘ì†
                full_url = f"{url}&pageNumber={current_page}"
                self.driver.get(full_url)

                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                if not self.wait_for_load():
                    print("âš ï¸ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨, ì¬ì‹œë„...")
                    time.sleep(2)
                    continue

                # í˜„ì¬ í˜ì´ì§€ì˜ ìƒí’ˆ í¬ë¡¤ë§
                if not self.crawl_page():
                    break

                # ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸
                end_page = self.get_end_page()

                if current_page >= end_page:
                    print(f"âœ“ ë§ˆì§€ë§‰ í˜ì´ì§€({end_page})ì— ë„ë‹¬")
                    break

                current_page += 1
                time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:  {e}")

        finally:
            self.quit_driver()

        print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ {len(self.product_list)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")

        return self.product_list

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê¸°ë³¸ ì‚¬ìš© (ë¸Œë¼ìš°ì € ì•ˆë³´ì„)
    crawler = SwimcapCrawler(headless=True)

    url = "https://swim.co.kr/categories/918606/products?childCategoryNo=919019&brands=%255B43160576%255D&pageNumber=1"

    product_list = crawler.crawl(url)

    # ê²°ê³¼ ì¶œë ¥
    for i, product in enumerate(product_list, 1):
        print(f"{i}. {product}")

# ë°°ëŸ´
# https://swim.co.kr/categories/918606/products?childCategoryNo=919019&brands=%255B43160576%255D&pageNumber=1

# í”¼ë‹‰ìŠ¤
# https://swim.co.kr/categories/918606/products?childCategoryNo=919019&brands=%255B43160578%255D